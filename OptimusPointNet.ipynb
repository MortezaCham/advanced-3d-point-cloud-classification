{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90MtVPoZ1vVb",
        "outputId": "c489c504-6432-44c3-b1a4-49f4e5363ee1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "  inflating: data/ModelNet40/monitor/train/monitor_0120.off  \n",
            "  inflating: data/ModelNet40/monitor/train/monitor_0199.off  \n",
            "  inflating: data/ModelNet40/monitor/train/monitor_0285.off  \n",
            "  shelf_0074.off  \n",
            "  inflating: data/ModelNet40/bookshelf/train/bookshelf_0276.off  \n",
            "  inflating: data/ModelNet40/bookshelf/train/bookshelf_0353.off  \n",
            "  inflating: data/ModelNet40/dresser/test/dresser_0284.off  \n",
            "  inflating: data/ModelNet40/dresser/test/dresser_0224.off  \n",
            "  inflating: data/ModelNet40/dresser/test/dresser_0266.off  \n",
            "  inflating: data/ModelNet40/dresser/test/dresser_0270.off  \n",
            "  inflating: data/ModelNet40/dresser/test/dresser_0253.off  \n",
            "  inflating: data/ModelNet40/dresser/test/dresser_0265.off  \n",
            "  inflating: data/ModelNet40/dresser/test/dresser_0228.off  \n",
            "  inflating: data/ModelNet40/dresser/test/dresser_0271.off  \n",
            "  inflating: data/ModelNet40/dresser/test/dresser_0215.off  \n",
            "  inflating: data/ModelNet40/dresser/test/dresser_0260.off  \n",
            "  inflating: data/ModelNet40/dresser/test/dresser_0243.off  \n",
            "  inflating: data/ModelNet40/dresser/test/dresser_0207.off  \n",
            "  inflating: data/ModelNet40/dresser/test/dresser_0211.off  \n",
            "  inflating: data/ModelNet40/dresser/test/dresser_0249.off  \n",
            "  inflating: data/ModelNet40/dresser/test/dresser_0221.off  \n",
            "  inflating: data/ModelNet40/dresser/test/dresser_0202.off  \n",
            "  inflating: data/ModelNet40/dresser/test/dresser_0263.off  \n",
            "  inflating: data/ModelNet40/dresser/test/dresser_0231.off  \n",
            "  inflating: data/ModelNet40/dresser/test/dresser_0242.off  \n",
            "  inflating: data/ModelNet40/dresser/test/dresser_0220.off  \n",
            "  inflating: data/ModelNet40/dresser/test/dresser_0274.off  \n",
            "  inflating: data/ModelNet40/dresser/test/dresser_0236.off  \n",
            "  inflating: data/ModelNet40/dresser/test/dresser_0201.off  \n",
            "  inflating: data/ModelNet40/dresser/test/dresser_0237.off  \n",
            "  inflating: data/ModelNet40/dresser/test/dresser_0281.off  \n",
            "  inflating: data/ModelNet40/dresser/test/dresser_0257.off  \n",
            "  inflating: data/ModelNet40/dresser/test/dresser_0280.off  \n",
            "  inflating: data/ModelNet40/dresser/test/dresser_0206.off  \n",
            "  inflating: data/ModelNet40/dresser/test/dresser_0275.off  \n",
            "  inflating: data/ModelNet40/dresser/test/dresser_0216.off  \n",
            "  inflating: data/ModelNet40/dresser/test/dresser_0235.off  \n",
            "  inflating: data/ModelNet40/dresser/test/dresser_0234.off  \n",
            "  inflating: data/ModelNet40/dresser/test/dresser_0225.off  \n",
            "  inflating: data/ModelNet40/dresser/test/dresser_0247.off  \n",
            "  inflating: data/ModelNet40/dresser/test/dresser_0245.off  \n",
            "  inflating: data/ModelNet40/dresser/test/dresser_0248.off  \n",
            "  inflating: data/ModelNet40/dresser/test/dresser_0229.off  \n",
            "  inflating: data/ModelNet40/dresser/test/dresser_0222.off  \n",
            "  inflating: data/ModelNet40/dresser/test/dresser_0258.off  \n",
            "  inflating: data/ModelNet40/dresser/test/dresser_0226.off  \n",
            "  inflating: data/ModelNet40/dresser/test/dresser_0238.off  \n",
            "  inflating: data/ModelNet40/dresser/test/dresser_0205.off  \n",
            "  inflating: data/ModelNet40/dresser/test/dresser_0209.off  \n",
            "  inflating: data/ModelNet40/dresser/test/dresser_0213.off  \n",
            "  inflating: data/ModelNet40/dresser/test/dresser_0267.off  \n",
            "  inflating: data/ModelNet40/dresser/test/dresser_0246.off  \n",
            "  inflating: data/ModelNet40/dresser/test/dresser_0261.off  \n",
            "  inflating: data/ModelNet40/dresser/test/dresser_0239.off  \n",
            "  inflating: data/ModelNet40/dresser/test/dresser_0218.off  \n",
            "  inflating: data/ModelNet40/dresser/test/dresser_0272.off  \n",
            "  inflating: data/ModelNet40/dresser/test/dresser_0259.off  \n",
            "  inflating: data/ModelNet40/dresser/test/dresser_0268.off  \n",
            "  inflating: data/ModelNet40/dresser/test/dresser_0232.off  \n",
            "  inflating: data/ModelNet40/dresser/test/dresser_0264.off  \n",
            "  inflating: data/ModelNet40/dresser/test/dresser_0250.off  \n",
            "  inflating: data/ModelNet40/dresser/test/dresser_0203.off  \n",
            "  inflating: data/ModelNet40/dresser/test/dresser_0277.off  \n",
            "  inflating: data/ModelNet40/dresser/test/dresser_0283.off  \n",
            "  inflating: data/ModelNet40/dresser/test/dresser_0279.off  \n",
            "  inflating: data/ModelNet40/dresser/test/dresser_0285.off  \n",
            "  inflating: data/ModelNet40/dresser/test/dresser_0252.off  \n",
            "  inflating: data/ModelNet40/dresser/test/dresser_0256.off  \n",
            "  inflating: data/ModelNet40/dresser/test/dresser_0217.off  \n",
            "  inflating: data/ModelNet40/dresser/test/dresser_0212.off  \n",
            "  inflating: data/ModelNet40/dresser/test/dresser_0208.off  \n",
            "  inflating: data/ModelNet40/dresser/test/dresser_0219.off  \n",
            "  inflating: data/ModelNet40/dresser/test/dresser_0273.off  \n",
            "  inflating: data/ModelNet40/dresser/test/dresser_0254.off  \n",
            "  inflating: data/ModelNet40/dresser/test/dresser_0233.off  \n",

            "  inflating: data/ModelNet40/dresser/test/dresser_0278.off  \n"
          ]
        }
      ],
      "source": [
        "# Install necessary libraries\n",
        "!pip install torch torchvision\n",
        "\n",
        "# Download ModelNet40 dataset (optional: You can directly upload the dataset to Google Drive or Colab)\n",
        "!wget https://modelnet.cs.princeton.edu/ModelNet40.zip\n",
        "!unzip ModelNet40.zip -d data/\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install trimesh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sCI_wriK2yZA",
        "outputId": "ea8006e9-403a-47b7-8978-7befbafb6629"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: trimesh in /usr/local/lib/python3.10/dist-packages (4.4.9)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from trimesh) (1.26.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import trimesh\n",
        "from torchvision import transforms\n",
        "\n",
        "# Set device to GPU if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
      ],
      "metadata": {
        "id": "6I1rEW8T2s4S"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PointCloudDataset(Dataset):\n",
        "    def __init__(self, root_dir, split='train', transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.split = split\n",
        "        self.transform = transform\n",
        "        self.file_paths = []\n",
        "        self.labels = []\n",
        "        self.categories = sorted(os.listdir(root_dir))\n",
        "        self.category_to_idx = {category: idx for idx, category in enumerate(self.categories)}\n",
        "\n",
        "        self.load_data()\n",
        "\n",
        "    def load_data(self):\n",
        "        for category in self.categories:\n",
        "            category_dir = os.path.join(self.root_dir, category, self.split)\n",
        "            if os.path.isdir(category_dir):\n",
        "                for file in os.listdir(category_dir):\n",
        "                    if file.endswith('.off'):\n",
        "                        self.file_paths.append(os.path.join(category_dir, file))\n",
        "                        self.labels.append(self.category_to_idx[category])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        file_path = self.file_paths[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        mesh = trimesh.load(file_path)\n",
        "        point_cloud = mesh.sample(768)  # Increased to 768 points\n",
        "\n",
        "        if self.transform:\n",
        "            point_cloud = self.transform(point_cloud)\n",
        "\n",
        "        point_cloud = point_cloud - point_cloud.mean(axis=0)\n",
        "        point_cloud = point_cloud / np.max(np.linalg.norm(point_cloud, axis=1))\n",
        "\n",
        "        return torch.from_numpy(point_cloud).float().transpose(0, 1), label\n",
        "\n"
      ],
      "metadata": {
        "id": "c9a9jebN24Fm"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RandomScale:\n",
        "    def __init__(self, scale_range=(0.8, 1.2)):\n",
        "        self.scale_range = scale_range\n",
        "\n",
        "    def __call__(self, point_cloud):\n",
        "        scale = np.random.uniform(*self.scale_range)\n",
        "        return point_cloud * scale\n",
        "\n",
        "class JitterPoints:\n",
        "    def __init__(self, sigma=0.01, clip=0.05):\n",
        "        self.sigma = sigma\n",
        "        self.clip = clip\n",
        "\n",
        "    def __call__(self, point_cloud):\n",
        "        jittered_data = np.clip(self.sigma * np.random.randn(*point_cloud.shape), -self.clip, self.clip)\n",
        "        return point_cloud + jittered_data\n",
        "\n",
        "transform = transforms.Compose([RandomRotation(), RandomScale(), JitterPoints()])\n"
      ],
      "metadata": {
        "id": "-W-jOgiQESKo"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import numpy as np\n",
        "import os\n",
        "import trimesh\n",
        "from torchvision import transforms\n",
        "from tqdm import tqdm  # For progress bars\n"
      ],
      "metadata": {
        "id": "012tKWlx553A"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 16\n",
        "\n",
        "train_dataset = PointCloudDataset(root_dir='data/ModelNet40', split='train', transform=transform)\n",
        "test_dataset = PointCloudDataset(root_dir='data/ModelNet40', split='test', transform=None)\n",
        "\n",
        "# Using only 1000 samples for training and 200 for testing (for debugging)\n",
        "train_dataset, _ = random_split(train_dataset, [1000, len(train_dataset) - 1000])\n",
        "test_dataset, _ = random_split(test_dataset, [200, len(test_dataset) - 200])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
      ],
      "metadata": {
        "id": "TClorlhC4OQx"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PointNet(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(PointNet, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(3, 64, kernel_size=1)\n",
        "        self.bn1 = nn.BatchNorm1d(64)  # Batch Normalization\n",
        "        self.conv2 = nn.Conv1d(64, 128, kernel_size=1)\n",
        "        self.bn2 = nn.BatchNorm1d(128)\n",
        "        self.conv3 = nn.Conv1d(128, 512, kernel_size=1)  # Increased to 512 channels\n",
        "        self.bn3 = nn.BatchNorm1d(512)\n",
        "\n",
        "        self.fc1 = nn.Linear(512, 256)\n",
        "        self.bn4 = nn.BatchNorm1d(256)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.bn5 = nn.BatchNorm1d(128)\n",
        "        self.fc3 = nn.Linear(128, num_classes)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(p=0.3)\n",
        "        self.log_softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.bn1(self.conv1(x)))\n",
        "        x = self.relu(self.bn2(self.conv2(x)))\n",
        "        x = self.relu(self.bn3(self.conv3(x)))\n",
        "        x = torch.max(x, 2)[0]  # Max pooling\n",
        "        x = self.relu(self.bn4(self.fc1(x)))\n",
        "        x = self.dropout(self.relu(self.bn5(self.fc2(x))))\n",
        "        x = self.fc3(x)\n",
        "        return self.log_softmax(x)\n"
      ],
      "metadata": {
        "id": "FCaIOgnf4OSI"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for data, target in tqdm(loader, desc=\"Training\"):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        pred = output.argmax(dim=1)\n",
        "        correct += pred.eq(target).sum().item()\n",
        "        total += target.size(0)\n",
        "\n",
        "    return total_loss / len(loader), correct / total\n",
        "\n",
        "def test(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, target in tqdm(loader, desc=\"Testing\"):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "            total_loss += loss.item()\n",
        "            pred = output.argmax(dim=1)\n",
        "            correct += pred.eq(target).sum().item()\n",
        "            total += target.size(0)\n",
        "\n",
        "    return total_loss / len(loader), correct / total\n"
      ],
      "metadata": {
        "id": "gRnuHd294Vs8"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = PointNet(num_classes=len(train_dataset.dataset.categories)).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0005, weight_decay=1e-4)  # Reduced learning rate, added weight decay\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "epochs = 20\n",
        "for epoch in range(epochs):\n",
        "    print(f'Epoch {epoch+1}/{epochs}')\n",
        "\n",
        "    train_loss, train_acc = train(model, train_loader, optimizer, criterion, device)\n",
        "    test_loss, test_acc = test(model, test_loader, criterion, device)\n",
        "    scheduler.step()\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{epochs} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | Test Loss: {test_loss:.4f} | Test Acc: {test_acc:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xWOoEhXe4Vvy",
        "outputId": "7bac8a5f-2f91-4fae-f428-3d75a6802813"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 63/63 [02:45<00:00,  2.63s/it]\n",
            "Testing: 100%|██████████| 13/13 [00:28<00:00,  2.22s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20 | Train Loss: 3.1588 | Train Acc: 0.2160 | Test Loss: 2.6465 | Test Acc: 0.2550\n",
            "Epoch 2/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 63/63 [02:40<00:00,  2.54s/it]\n",
            "Testing: 100%|██████████| 13/13 [00:28<00:00,  2.19s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/20 | Train Loss: 2.4686 | Train Acc: 0.3840 | Test Loss: 2.4841 | Test Acc: 0.3650\n",
            "Epoch 3/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 63/63 [02:39<00:00,  2.53s/it]\n",
            "Testing: 100%|██████████| 13/13 [00:27<00:00,  2.14s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/20 | Train Loss: 2.1402 | Train Acc: 0.4680 | Test Loss: 2.1694 | Test Acc: 0.4150\n",
            "Epoch 4/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 63/63 [02:39<00:00,  2.53s/it]\n",
            "Testing: 100%|██████████| 13/13 [00:27<00:00,  2.15s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/20 | Train Loss: 1.9088 | Train Acc: 0.5160 | Test Loss: 2.0333 | Test Acc: 0.4350\n",
            "Epoch 5/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 63/63 [02:40<00:00,  2.55s/it]\n",
            "Testing: 100%|██████████| 13/13 [00:28<00:00,  2.20s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/20 | Train Loss: 1.7501 | Train Acc: 0.5500 | Test Loss: 1.7743 | Test Acc: 0.4950\n",
            "Epoch 6/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 63/63 [02:39<00:00,  2.53s/it]\n",
            "Testing: 100%|██████████| 13/13 [00:28<00:00,  2.16s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/20 | Train Loss: 1.5579 | Train Acc: 0.5800 | Test Loss: 1.7697 | Test Acc: 0.4550\n",
            "Epoch 7/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 63/63 [02:39<00:00,  2.53s/it]\n",
            "Testing: 100%|██████████| 13/13 [00:27<00:00,  2.13s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/20 | Train Loss: 1.4479 | Train Acc: 0.6120 | Test Loss: 1.5384 | Test Acc: 0.6000\n",
            "Epoch 8/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 63/63 [02:38<00:00,  2.51s/it]\n",
            "Testing: 100%|██████████| 13/13 [00:27<00:00,  2.12s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/20 | Train Loss: 1.3583 | Train Acc: 0.6450 | Test Loss: 1.5261 | Test Acc: 0.6050\n",
            "Epoch 9/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 63/63 [02:38<00:00,  2.52s/it]\n",
            "Testing: 100%|██████████| 13/13 [00:28<00:00,  2.18s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/20 | Train Loss: 1.2690 | Train Acc: 0.6590 | Test Loss: 1.5558 | Test Acc: 0.5800\n",
            "Epoch 10/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 63/63 [02:38<00:00,  2.52s/it]\n",
            "Testing: 100%|██████████| 13/13 [00:27<00:00,  2.13s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/20 | Train Loss: 1.1671 | Train Acc: 0.6900 | Test Loss: 1.3772 | Test Acc: 0.6450\n",
            "Epoch 11/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 63/63 [02:38<00:00,  2.52s/it]\n",
            "Testing: 100%|██████████| 13/13 [00:27<00:00,  2.13s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11/20 | Train Loss: 1.0981 | Train Acc: 0.7050 | Test Loss: 1.2286 | Test Acc: 0.6850\n",
            "Epoch 12/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 63/63 [02:38<00:00,  2.51s/it]\n",
            "Testing: 100%|██████████| 13/13 [00:27<00:00,  2.15s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12/20 | Train Loss: 1.0298 | Train Acc: 0.7390 | Test Loss: 1.1964 | Test Acc: 0.6900\n",
            "Epoch 13/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 63/63 [02:38<00:00,  2.52s/it]\n",
            "Testing: 100%|██████████| 13/13 [00:28<00:00,  2.18s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13/20 | Train Loss: 1.0053 | Train Acc: 0.7330 | Test Loss: 1.1995 | Test Acc: 0.6900\n",
            "Epoch 14/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 63/63 [02:38<00:00,  2.51s/it]\n",
            "Testing: 100%|██████████| 13/13 [00:27<00:00,  2.12s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14/20 | Train Loss: 0.9681 | Train Acc: 0.7340 | Test Loss: 1.2208 | Test Acc: 0.6850\n",
            "Epoch 15/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 63/63 [02:39<00:00,  2.52s/it]\n",
            "Testing: 100%|██████████| 13/13 [00:27<00:00,  2.11s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15/20 | Train Loss: 0.9306 | Train Acc: 0.7430 | Test Loss: 1.1875 | Test Acc: 0.6650\n",
            "Epoch 16/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 63/63 [02:38<00:00,  2.51s/it]\n",
            "Testing: 100%|██████████| 13/13 [00:27<00:00,  2.13s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16/20 | Train Loss: 0.9139 | Train Acc: 0.7490 | Test Loss: 1.1430 | Test Acc: 0.6950\n",
            "Epoch 17/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 63/63 [02:38<00:00,  2.51s/it]\n",
            "Testing: 100%|██████████| 13/13 [00:28<00:00,  2.16s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17/20 | Train Loss: 0.9199 | Train Acc: 0.7440 | Test Loss: 1.2168 | Test Acc: 0.6700\n",
            "Epoch 18/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 63/63 [02:37<00:00,  2.50s/it]\n",
            "Testing: 100%|██████████| 13/13 [00:28<00:00,  2.16s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18/20 | Train Loss: 0.8417 | Train Acc: 0.7740 | Test Loss: 1.0544 | Test Acc: 0.7150\n",
            "Epoch 19/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 63/63 [02:38<00:00,  2.52s/it]\n",
            "Testing: 100%|██████████| 13/13 [00:27<00:00,  2.11s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19/20 | Train Loss: 0.8383 | Train Acc: 0.7720 | Test Loss: 1.0835 | Test Acc: 0.7100\n",
            "Epoch 20/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 63/63 [02:37<00:00,  2.51s/it]\n",
            "Testing: 100%|██████████| 13/13 [00:27<00:00,  2.10s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20/20 | Train Loss: 0.8126 | Train Acc: 0.7740 | Test Loss: 1.1122 | Test Acc: 0.7050\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}
